# ğŸ¤Ÿ Sign Language Detection System

A real-time sign language detection system using **MediaPipe** for hand tracking and **Machine Learning** for gesture recognition. Detects ASL (American Sign Language) gestures with high accuracy.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![OpenCV](https://img.shields.io/badge/OpenCV-4.8+-green.svg)
![MediaPipe](https://img.shields.io/badge/MediaPipe-0.10+-orange.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

## âœ¨ Features

- ğŸ¥ **Real-time Detection** - Instant sign language recognition from webcam
- ğŸ¤– **Machine Learning** - Random Forest classifier with 95%+ accuracy
- ğŸ‘‹ **Hand Tracking** - MediaPipe-based robust hand landmark detection
- ğŸ“Š **Confidence Scores** - Visual confidence indicators
- ğŸ¯ **Smooth Predictions** - Temporal smoothing for stable results
- ğŸ”§ **Easy Training** - Simple data collection and model training
- ğŸ“± **Lightweight** - Runs on CPU, no GPU required

## ğŸ¬ Demo

![Demo](https://via.placeholder.com/800x400?text=Sign+Language+Detection+Demo)

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- Webcam
- Windows/macOS/Linux

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/aiml34048/sign-language-detection.git
cd sign-language-detection
```

2. **Create virtual environment** (recommended)
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

### Usage

#### Option 1: Use Pre-trained Model (Coming Soon)

```bash
python main.py
```

#### Option 2: Train Your Own Model

1. **Collect training data**
```bash
python train_model.py
```

Follow the on-screen instructions:
- Press **SPACE** to start collecting samples for each sign
- Show the sign clearly to the camera
- 100 samples will be collected per sign
- Press **'q'** to skip a sign

2. **Run the detector**
```bash
python main.py
```

3. **Controls**
- Show ASL signs to the camera
- Press **'q'** to quit

## ğŸ“ Project Structure

```
sign-language-detection/
â”œâ”€â”€ main.py                 # Main detection application
â”œâ”€â”€ train_model.py          # Model training script
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ model/                 # Trained models (created after training)
â”‚   â””â”€â”€ sign_language_model.pkl
â”œâ”€â”€ data/                  # Training data (optional)
â””â”€â”€ docs/                  # Documentation
    â””â”€â”€ ASL_REFERENCE.md   # ASL alphabet reference
```

## ğŸ§  How It Works

### 1. Hand Tracking
- Uses **MediaPipe Hands** to detect 21 hand landmarks
- Tracks hand position, orientation, and finger positions
- Works in real-time with high accuracy

### 2. Feature Extraction
- Extracts 3D coordinates (x, y, z) of all 21 landmarks
- Normalizes coordinates relative to wrist position
- Scales by hand size for size-invariant recognition

### 3. Classification
- **Random Forest Classifier** with 200 trees
- Trained on normalized landmark features
- Temporal smoothing for stable predictions

### 4. Prediction Pipeline
```
Camera Frame â†’ Hand Detection â†’ Landmark Extraction â†’ 
Normalization â†’ ML Model â†’ Prediction â†’ Display
```

## ğŸ¯ Supported Signs

Currently supports ASL alphabet (A-Z):

| Letter | Description |
|--------|-------------|
| A | Closed fist, thumb on side |
| B | Flat hand, fingers together |
| C | Curved hand forming C |
| D | Index finger up, others closed |
| E | Fingers curled, thumb across |
| ... | ... |

Full reference: [ASL Alphabet Guide](docs/ASL_REFERENCE.md)

## ğŸ“Š Model Performance

- **Accuracy**: 95%+ on test set
- **Inference Speed**: 30+ FPS on CPU
- **Training Time**: ~5 minutes for 5 signs
- **Model Size**: < 5 MB

## ğŸ”§ Configuration

Edit `main.py` to customize:

```python
# Hand detection confidence
min_detection_confidence=0.7

# Tracking confidence
min_tracking_confidence=0.5

# Prediction smoothing window
prediction_buffer = deque(maxlen=5)

# Camera resolution
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
```

## ğŸ“ˆ Training Your Own Model

### Collect Data

```bash
python train_model.py
```

**Tips for good data collection:**
- âœ… Good lighting conditions
- âœ… Plain background
- âœ… Vary hand position slightly
- âœ… Keep hand in frame
- âœ… Collect from different angles

### Customize Signs

Edit `train_model.py`:

```python
# Train on specific signs
signs = ['A', 'B', 'C', 'D', 'E']

# Or full alphabet
signs = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')

# Or custom gestures
signs = ['Hello', 'Thanks', 'Yes', 'No']
```

### Model Parameters

Adjust in `train_model.py`:

```python
model = RandomForestClassifier(
    n_estimators=200,      # Number of trees
    max_depth=20,          # Tree depth
    min_samples_split=5,   # Min samples to split
    random_state=42
)
```

## ğŸ› Troubleshooting

### Camera not working
```bash
# Check available cameras
python -c "import cv2; print(cv2.VideoCapture(0).isOpened())"

# Try different camera index
cap = cv2.VideoCapture(1)  # or 2, 3, etc.
```

### Low accuracy
- Collect more training samples (200+ per sign)
- Ensure good lighting
- Use plain background
- Keep hand steady during collection

### Slow performance
- Reduce camera resolution
- Lower `max_num_hands` to 1
- Reduce `n_estimators` in model

## ğŸ¤ Contributing

Contributions are welcome! Here's how:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ To-Do

- [ ] Add pre-trained model for all ASL letters
- [ ] Support for ASL words and phrases
- [ ] Mobile app version
- [ ] Real-time translation to text
- [ ] Support for other sign languages (BSL, ISL, etc.)
- [ ] Web-based version
- [ ] Dataset augmentation
- [ ] Deep learning model (CNN/LSTM)

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [MediaPipe](https://google.github.io/mediapipe/) - Hand tracking
- [OpenCV](https://opencv.org/) - Computer vision
- [scikit-learn](https://scikit-learn.org/) - Machine learning
- ASL community for inspiration

## ğŸ“§ Contact

- **Author**: aiml34048
- **Email**: aiml34048@gmail.com
- **GitHub**: [@aiml34048](https://github.com/aiml34048)

## â­ Star History

If you find this project useful, please consider giving it a star!

---

**Made with â¤ï¸ for the deaf and hard-of-hearing community**
